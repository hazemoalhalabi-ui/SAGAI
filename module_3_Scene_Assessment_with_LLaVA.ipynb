{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1N2vNjuZeDCmIXQFF1xcrTTQ4I5kvJ9P-","timestamp":1744900479926}],"gpuType":"T4","authorship_tag":"ABX9TyNZrZKk3iPDHPre0qVAISNN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"50636a4024c04d479878c6be351763b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2b7756953f54efe9a1bc5c42da276b9","IPY_MODEL_8c3b8e9b60fe4462865dfeb5e14e9ec0","IPY_MODEL_76b2f46ebe4645458985126e7e63dfed"],"layout":"IPY_MODEL_853913df5c6447779d4aa34195d0583c"}},"f2b7756953f54efe9a1bc5c42da276b9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90795a55239c4b75ae1d2d21cb23bbe7","placeholder":"​","style":"IPY_MODEL_000280726eb04a7e832f1a28ba2b1221","value":"Fetching 15 files: 100%"}},"8c3b8e9b60fe4462865dfeb5e14e9ec0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ae89d705c4458384bb848176bc74c8","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89ffdd727aec4c78a7bd8a65dbab539f","value":15}},"76b2f46ebe4645458985126e7e63dfed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_845488df38f3481dbcff8f3a6524997a","placeholder":"​","style":"IPY_MODEL_026f3fa09cd84c5ca73bcffa8c2f82ad","value":" 15/15 [03:31&lt;00:00, 26.12s/it]"}},"853913df5c6447779d4aa34195d0583c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90795a55239c4b75ae1d2d21cb23bbe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"000280726eb04a7e832f1a28ba2b1221":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3ae89d705c4458384bb848176bc74c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ffdd727aec4c78a7bd8a65dbab539f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"845488df38f3481dbcff8f3a6524997a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"026f3fa09cd84c5ca73bcffa8c2f82ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d1043a152dc4070b78bc19077672367":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0b80960820d44d4a765f03c2c9cf9cf","IPY_MODEL_4c4920ed0de74190a1ee5b4786988e59","IPY_MODEL_02a7ec249f5f49579ff199b86e979a95"],"layout":"IPY_MODEL_ae7147790ac8439983e422c722255f3f"}},"b0b80960820d44d4a765f03c2c9cf9cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05ded39522ad47b69e003c65bf17f8d9","placeholder":"​","style":"IPY_MODEL_a68efc5e55f74028b21bb0abb9ee3d6d","value":"model-00001-of-00004.safetensors: 100%"}},"4c4920ed0de74190a1ee5b4786988e59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b48c82398f164c6a8d471ddb981a52fd","max":4943170624,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdb5ff6923a64656b259a3428a8817c5","value":4943170624}},"02a7ec249f5f49579ff199b86e979a95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2affa17c43b14e38874c787817425273","placeholder":"​","style":"IPY_MODEL_41d357b5a6ff4c1e9e829c2f4bac2a4d","value":" 4.94G/4.94G [03:29&lt;00:00, 49.3MB/s]"}},"ae7147790ac8439983e422c722255f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05ded39522ad47b69e003c65bf17f8d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a68efc5e55f74028b21bb0abb9ee3d6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b48c82398f164c6a8d471ddb981a52fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb5ff6923a64656b259a3428a8817c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2affa17c43b14e38874c787817425273":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41d357b5a6ff4c1e9e829c2f4bac2a4d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ac9c1803eed444d93d13da60934484c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fdded508c8d14bedbcb04fdd7653e033","IPY_MODEL_e78156ba1016460eaa212d38433a843e","IPY_MODEL_31c12294cb4a4551afa10f724b4f78c9"],"layout":"IPY_MODEL_bf4a6c798411424e9b9073dc562eef73"}},"fdded508c8d14bedbcb04fdd7653e033":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_650f9eabf5d34fe8a69ca4b40bf76d28","placeholder":"​","style":"IPY_MODEL_cc19a044a9b44af194cd575281507951","value":"config.json: 100%"}},"e78156ba1016460eaa212d38433a843e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a272646edda438fae53504bb9de4c45","max":1629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0597573c70284bb3b1da892aaea1d28d","value":1629}},"31c12294cb4a4551afa10f724b4f78c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_341d8026b2024164a087277225055db6","placeholder":"​","style":"IPY_MODEL_df7a7dad83294f82b1a466d25e02fc36","value":" 1.63k/1.63k [00:00&lt;00:00, 109kB/s]"}},"bf4a6c798411424e9b9073dc562eef73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"650f9eabf5d34fe8a69ca4b40bf76d28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc19a044a9b44af194cd575281507951":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a272646edda438fae53504bb9de4c45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0597573c70284bb3b1da892aaea1d28d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"341d8026b2024164a087277225055db6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df7a7dad83294f82b1a466d25e02fc36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd6c656f09bf4b198c7dd9691472eff0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_96a40eae71ca4d11a96ae096386c04c2","IPY_MODEL_4819381fefe44c59bbbec85aeba16943","IPY_MODEL_495de7a018cc42b1a03db0fa47c3b9ae"],"layout":"IPY_MODEL_64fce670b1c64cca9375dfb8f1cc9423"}},"96a40eae71ca4d11a96ae096386c04c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_839a3f251cc848a99d7b631fe52566a2","placeholder":"​","style":"IPY_MODEL_a819db7b0fbe4cf582bcfafaf2c3fcc6","value":"README.md: 100%"}},"4819381fefe44c59bbbec85aeba16943":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_099d29749fb24c54b1e412ca5b3f55da","max":1525,"min":0,"orientation":"horizontal","style":"IPY_MODEL_300c7f8c64ad49b0847e5b287de26d90","value":1525}},"495de7a018cc42b1a03db0fa47c3b9ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9422d76bb4e460f8b6e70868fcc79ed","placeholder":"​","style":"IPY_MODEL_407e21c2647e49ee94fd1624ef01890e","value":" 1.52k/1.52k [00:00&lt;00:00, 44.1kB/s]"}},"64fce670b1c64cca9375dfb8f1cc9423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"839a3f251cc848a99d7b631fe52566a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a819db7b0fbe4cf582bcfafaf2c3fcc6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"099d29749fb24c54b1e412ca5b3f55da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"300c7f8c64ad49b0847e5b287de26d90":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9422d76bb4e460f8b6e70868fcc79ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"407e21c2647e49ee94fd1624ef01890e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3cfa3c5801648bb965c720e66ee0966":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53a5334eb76f448c8571eb69c157e91c","IPY_MODEL_d317b38fb99844a9b386cd515aab93ac","IPY_MODEL_6885e431881048ef856121eb18002d84"],"layout":"IPY_MODEL_1b6b054b23a84abea8e2e1ec4ac3c486"}},"53a5334eb76f448c8571eb69c157e91c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_492a8cc500ec489bb0766478fbd91a7d","placeholder":"​","style":"IPY_MODEL_faf8de4f36df46288d532d6cfbf6aaa0","value":"generation_config.json: 100%"}},"d317b38fb99844a9b386cd515aab93ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06ac742458a84938adac0c5aee3eadb2","max":111,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2884e5a46549494ab747c327590f2fdd","value":111}},"6885e431881048ef856121eb18002d84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2e9e481b2d44ccc93122e7e591d4781","placeholder":"​","style":"IPY_MODEL_c8ba7ed43ebf474a96a39f82cdbb69fd","value":" 111/111 [00:00&lt;00:00, 2.07kB/s]"}},"1b6b054b23a84abea8e2e1ec4ac3c486":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492a8cc500ec489bb0766478fbd91a7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faf8de4f36df46288d532d6cfbf6aaa0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06ac742458a84938adac0c5aee3eadb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2884e5a46549494ab747c327590f2fdd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d2e9e481b2d44ccc93122e7e591d4781":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8ba7ed43ebf474a96a39f82cdbb69fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"147eaf968dc44ccdb532360a533b57fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf3c918db85a4608a12fc562082bb39d","IPY_MODEL_86af250fcd5b4b9293893423525a5b7b","IPY_MODEL_d896d61704e844f4a0a88817395a4648"],"layout":"IPY_MODEL_f7012699ae4a44ce8d447568c165409f"}},"cf3c918db85a4608a12fc562082bb39d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20157737fe8e4b7a8ca0068a1313b7be","placeholder":"​","style":"IPY_MODEL_f837ac64886f4bc6a270207fe3953d92","value":"model-00003-of-00004.safetensors: 100%"}},"86af250fcd5b4b9293893423525a5b7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d228849e97824b8d93f686b1d787b990","max":4927408360,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4df3e5a94b104831aa2f5300789a899c","value":4927408360}},"d896d61704e844f4a0a88817395a4648":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ddf75f990e94d3d9fc2d8cae5281ee4","placeholder":"​","style":"IPY_MODEL_ae946acd5c7a482eb1ca0b586e7675f9","value":" 4.93G/4.93G [03:31&lt;00:00, 87.4MB/s]"}},"f7012699ae4a44ce8d447568c165409f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20157737fe8e4b7a8ca0068a1313b7be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f837ac64886f4bc6a270207fe3953d92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d228849e97824b8d93f686b1d787b990":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4df3e5a94b104831aa2f5300789a899c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ddf75f990e94d3d9fc2d8cae5281ee4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae946acd5c7a482eb1ca0b586e7675f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd584dd7447343b985823295b8addd57":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab2e6463698040d5b76cfc46ca2ed25e","IPY_MODEL_d6a52ecc482a49fba8f195b78fbd6b8a","IPY_MODEL_899733af9d4b462fbee0b403a1e8285a"],"layout":"IPY_MODEL_f14f76aa115f4ba9a9ae3634d117021a"}},"ab2e6463698040d5b76cfc46ca2ed25e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5d597e162ae4981ac4031ab13c3f350","placeholder":"​","style":"IPY_MODEL_4484ee3dfe4d46e1a0026cc7c5b9e621","value":"model-00002-of-00004.safetensors: 100%"}},"d6a52ecc482a49fba8f195b78fbd6b8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b682a8863c14943a60cf4b7faf71c66","max":4999819336,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8fad3b2086184033ae5be47b3b46284d","value":4999819336}},"899733af9d4b462fbee0b403a1e8285a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1452c85569b24d55baf3e8c3077dc036","placeholder":"​","style":"IPY_MODEL_2b7adb1a804744a991abbdb3a9a07444","value":" 5.00G/5.00G [03:29&lt;00:00, 72.4MB/s]"}},"f14f76aa115f4ba9a9ae3634d117021a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5d597e162ae4981ac4031ab13c3f350":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4484ee3dfe4d46e1a0026cc7c5b9e621":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b682a8863c14943a60cf4b7faf71c66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fad3b2086184033ae5be47b3b46284d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1452c85569b24d55baf3e8c3077dc036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b7adb1a804744a991abbdb3a9a07444":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38c186c6b7494f539de491ba67f8b412":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3b1a7cf65e84186bcd4dbb26cb2a939","IPY_MODEL_83d99181700249688187494a00d705c0","IPY_MODEL_dab80ba46229491baa86c41fe6a3cf2f"],"layout":"IPY_MODEL_075f8d1447bb463193456791f7717489"}},"f3b1a7cf65e84186bcd4dbb26cb2a939":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_105b970eceb745f6a6b3158d6f9f0700","placeholder":"​","style":"IPY_MODEL_1284f2b225a04535ba4b930ccfe65108","value":"model-00004-of-00004.safetensors: 100%"}},"83d99181700249688187494a00d705c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_90476f3a082c41c581681b81bcbad962","max":262144128,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a6d74cdc900401fa0c30a2f558e06a3","value":262144128}},"dab80ba46229491baa86c41fe6a3cf2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fc74588742e406b946a9251f87a2154","placeholder":"​","style":"IPY_MODEL_2cc9b798d1bd4fc2bc1b49c22d1105dd","value":" 262M/262M [00:06&lt;00:00, 68.0MB/s]"}},"075f8d1447bb463193456791f7717489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"105b970eceb745f6a6b3158d6f9f0700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1284f2b225a04535ba4b930ccfe65108":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90476f3a082c41c581681b81bcbad962":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a6d74cdc900401fa0c30a2f558e06a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4fc74588742e406b946a9251f87a2154":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2cc9b798d1bd4fc2bc1b49c22d1105dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ca5cb317f7f42578dc09ba66f7003bc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3126b38360a340d4affcbf46119a4190","IPY_MODEL_7e1d709f38e5417d9c8e35019506011a","IPY_MODEL_8fbff9a004b040de9231229c2b93294a"],"layout":"IPY_MODEL_51e669e68eaf4d30899bb7b75dcc4369"}},"3126b38360a340d4affcbf46119a4190":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e753e715a8cf46629a7a6f187396a256","placeholder":"​","style":"IPY_MODEL_c33b337c48654d1d828b3fddf6d76893","value":".gitattributes: 100%"}},"7e1d709f38e5417d9c8e35019506011a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ee1d65a2990465fbcf2bd6442f67389","max":1519,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4c2026a284134ca7b27ffc1403d9c9a8","value":1519}},"8fbff9a004b040de9231229c2b93294a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98775e8301d74ee7b5020121e0fa45a4","placeholder":"​","style":"IPY_MODEL_4e3d924660e544d5b70c34321d3c57b0","value":" 1.52k/1.52k [00:00&lt;00:00, 12.3kB/s]"}},"51e669e68eaf4d30899bb7b75dcc4369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e753e715a8cf46629a7a6f187396a256":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c33b337c48654d1d828b3fddf6d76893":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ee1d65a2990465fbcf2bd6442f67389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c2026a284134ca7b27ffc1403d9c9a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98775e8301d74ee7b5020121e0fa45a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3d924660e544d5b70c34321d3c57b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c07e9e75c215489baf5472807776b4ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f51acd090c32452ab27954101099a8ff","IPY_MODEL_0898e438e5454f58ad2e9238ee48a5cc","IPY_MODEL_f6d11992b2594987b6336ec4470fe8ba"],"layout":"IPY_MODEL_28c55f0686e04a38959d1993761eb07e"}},"f51acd090c32452ab27954101099a8ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c66c7abc3914f21b19e000fe7bf2a25","placeholder":"​","style":"IPY_MODEL_5a4f8fc215bf4294bc4dbc71413272bf","value":"model.safetensors.index.json: 100%"}},"0898e438e5454f58ad2e9238ee48a5cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05338600ca854e1290639be39f35796c","max":73215,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a32ba3cb5b7c452fabcd311c92bc1db9","value":73215}},"f6d11992b2594987b6336ec4470fe8ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a5538c5e68a4cedaed4cb8a526a64d3","placeholder":"​","style":"IPY_MODEL_034fc232290e4ee6b1492e47e1f76680","value":" 73.2k/73.2k [00:00&lt;00:00, 646kB/s]"}},"28c55f0686e04a38959d1993761eb07e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c66c7abc3914f21b19e000fe7bf2a25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4f8fc215bf4294bc4dbc71413272bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05338600ca854e1290639be39f35796c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32ba3cb5b7c452fabcd311c92bc1db9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a5538c5e68a4cedaed4cb8a526a64d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"034fc232290e4ee6b1492e47e1f76680":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acb9d669653749869038f3ed25fbcc22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a70c84be45e74dc282e45d91bb97ea6a","IPY_MODEL_5b8d20cb53f54e978fafeed5431732e9","IPY_MODEL_4aee1faa206d46239c9e528eba4e5073"],"layout":"IPY_MODEL_de0c778950944eebbc3bba1515a71fca"}},"a70c84be45e74dc282e45d91bb97ea6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54337a2b77534223b0497ffc2b83aab9","placeholder":"​","style":"IPY_MODEL_a5af955d02bf43a4b10a800e22fac2b4","value":"tokenizer.json: 100%"}},"5b8d20cb53f54e978fafeed5431732e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2adec48a433a4d7bac0f949cf4cf5462","max":1795303,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd702378f8254418a8fe1c6a1722e8f7","value":1795303}},"4aee1faa206d46239c9e528eba4e5073":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b4eae7b55b34979aeac6914ca05d42e","placeholder":"​","style":"IPY_MODEL_b1602c7388b841e39e7c4a11a7b27a77","value":" 1.80M/1.80M [00:00&lt;00:00, 6.10MB/s]"}},"de0c778950944eebbc3bba1515a71fca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54337a2b77534223b0497ffc2b83aab9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5af955d02bf43a4b10a800e22fac2b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2adec48a433a4d7bac0f949cf4cf5462":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd702378f8254418a8fe1c6a1722e8f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b4eae7b55b34979aeac6914ca05d42e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1602c7388b841e39e7c4a11a7b27a77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6cd9babfe864958ab4c489d70054f15":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42494d4b673f447f9d735c35cc3c799f","IPY_MODEL_20e1bdeff8ce4337abdde2d5eb388fe0","IPY_MODEL_ab8928d5727c4afbbbcad5e2410cc670"],"layout":"IPY_MODEL_f12004307d394ca68aacd0f5848a38c6"}},"42494d4b673f447f9d735c35cc3c799f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d4864d0096a4b86ab8a623b22912ec7","placeholder":"​","style":"IPY_MODEL_311196ea168043bb9dbe0963f6803403","value":"special_tokens_map.json: 100%"}},"20e1bdeff8ce4337abdde2d5eb388fe0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e321851490d24e7896fbde4ebb5e9b08","max":438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c5914c36dc3402abdd5fcb9a4e8014c","value":438}},"ab8928d5727c4afbbbcad5e2410cc670":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0935f2f55fa44bb4915990b16f29e7c4","placeholder":"​","style":"IPY_MODEL_7262a607c3ef4b96b934b3ddd2c243c7","value":" 438/438 [00:00&lt;00:00, 3.89kB/s]"}},"f12004307d394ca68aacd0f5848a38c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d4864d0096a4b86ab8a623b22912ec7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"311196ea168043bb9dbe0963f6803403":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e321851490d24e7896fbde4ebb5e9b08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c5914c36dc3402abdd5fcb9a4e8014c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0935f2f55fa44bb4915990b16f29e7c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7262a607c3ef4b96b934b3ddd2c243c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2192e15597874e34ace4b95403d54989":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_592c40ccfd52456da025f6bf027c39f4","IPY_MODEL_99380a4b2f924dd78458933194e43d9b","IPY_MODEL_84085e22df164fb88be71c23abb6c397"],"layout":"IPY_MODEL_cba1f91d78c34dc0b9409d4a32c5e1f1"}},"592c40ccfd52456da025f6bf027c39f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30b5f40b8bc9498690004a74497cd588","placeholder":"​","style":"IPY_MODEL_dd1bc4fdbfac4f03a9e8a2488c713b5e","value":"tokenizer.model: 100%"}},"99380a4b2f924dd78458933194e43d9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a12bd35fdd3f4bb4ba54e1bfa168237f","max":493443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72fdfd6f1ed14c809f9bb70a26c1608e","value":493443}},"84085e22df164fb88be71c23abb6c397":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_754625e390d449a6bae4c691332b5dbc","placeholder":"​","style":"IPY_MODEL_1f0cdc0366b1493aa175f3cbbb198b54","value":" 493k/493k [00:00&lt;00:00, 2.05MB/s]"}},"cba1f91d78c34dc0b9409d4a32c5e1f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30b5f40b8bc9498690004a74497cd588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd1bc4fdbfac4f03a9e8a2488c713b5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a12bd35fdd3f4bb4ba54e1bfa168237f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72fdfd6f1ed14c809f9bb70a26c1608e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"754625e390d449a6bae4c691332b5dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f0cdc0366b1493aa175f3cbbb198b54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"264825367b4b4a3b96723d5016d78d25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aecd05b91061468f961a776d58a045ef","IPY_MODEL_a956c28fbd3e4c08b1c0a5954167a8cf","IPY_MODEL_2991fc08b535483f8c02f2afde23e1dc"],"layout":"IPY_MODEL_6b44e9e7645546508af4eedbc4c2e50a"}},"aecd05b91061468f961a776d58a045ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17310ac1a63643099b9a56e9839be005","placeholder":"​","style":"IPY_MODEL_52e179ef704846e68423ed63c57c16ba","value":"tokenizer_config.json: 100%"}},"a956c28fbd3e4c08b1c0a5954167a8cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_009a9b2ce341411bbbb488b22e38b13a","max":1462,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50f185dd759549358abe7365e89d19e2","value":1462}},"2991fc08b535483f8c02f2afde23e1dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b40eb985841a4276a61b933f3d51805a","placeholder":"​","style":"IPY_MODEL_3069e22cfc944262b408d1a0eaafeff2","value":" 1.46k/1.46k [00:00&lt;00:00, 22.1kB/s]"}},"6b44e9e7645546508af4eedbc4c2e50a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17310ac1a63643099b9a56e9839be005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e179ef704846e68423ed63c57c16ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"009a9b2ce341411bbbb488b22e38b13a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50f185dd759549358abe7365e89d19e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b40eb985841a4276a61b933f3d51805a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3069e22cfc944262b408d1a0eaafeff2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e523ad4d9a8498eac1184c75c600d23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5f30b7158214b8e8329c4d032f2617f","IPY_MODEL_f62b4f4c103240aaa9024e495d75bd3e","IPY_MODEL_3fb3b925fa74450d919db88a6696b6ef"],"layout":"IPY_MODEL_396a2368fdcc4fb9aa76564b752927fa"}},"b5f30b7158214b8e8329c4d032f2617f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adcc1d52a0294116aefb724daba5edac","placeholder":"​","style":"IPY_MODEL_8ed37aae288145149123cd3821999791","value":"trainer_state.json: 100%"}},"f62b4f4c103240aaa9024e495d75bd3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9257d191dc84410cb8435d8f716e6d88","max":718576,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce747633e4064f0c87f083d59f1214e0","value":718576}},"3fb3b925fa74450d919db88a6696b6ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e9c200c8a0a44dcbb5b1d28cf47c4dc","placeholder":"​","style":"IPY_MODEL_794a1a5cd86245bcb8837264de5b14b8","value":" 719k/719k [00:00&lt;00:00, 2.98MB/s]"}},"396a2368fdcc4fb9aa76564b752927fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adcc1d52a0294116aefb724daba5edac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ed37aae288145149123cd3821999791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9257d191dc84410cb8435d8f716e6d88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce747633e4064f0c87f083d59f1214e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e9c200c8a0a44dcbb5b1d28cf47c4dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"794a1a5cd86245bcb8837264de5b14b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b6c1da480b84f76bc17275379ec174a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f0f0ad41d96d482995a1d9d99ba2a272","IPY_MODEL_c6bbb3b27dde4970bf3f4728a339512d","IPY_MODEL_ed6b9ef4f20d4f23a1775bb0a984b7cc"],"layout":"IPY_MODEL_8efc5cdc20784ac5b8693338a30978c8"}},"f0f0ad41d96d482995a1d9d99ba2a272":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da9c0897fecd415c8c52ab07bfcc26b8","placeholder":"​","style":"IPY_MODEL_cf10b96dec594a60989067ad3ad20242","value":"training_args.bin: 100%"}},"c6bbb3b27dde4970bf3f4728a339512d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c976f7b557884d15afd4b8ed1269fbd8","max":7035,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11e6603b0dde4e9b8e2c50fd26d3b9f3","value":7035}},"ed6b9ef4f20d4f23a1775bb0a984b7cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e7217f9ca55461aa48de4342a2f6d90","placeholder":"​","style":"IPY_MODEL_6218a4e0aa854d8db2830db48827cca8","value":" 7.04k/7.04k [00:00&lt;00:00, 87.3kB/s]"}},"8efc5cdc20784ac5b8693338a30978c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da9c0897fecd415c8c52ab07bfcc26b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf10b96dec594a60989067ad3ad20242":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c976f7b557884d15afd4b8ed1269fbd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11e6603b0dde4e9b8e2c50fd26d3b9f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e7217f9ca55461aa48de4342a2f6d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6218a4e0aa854d8db2830db48827cca8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28b055d0a0f545f2b83ec412ef747da2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e88c8956761649f187a75c5edd83b985","IPY_MODEL_8fbee034c0444fac9431fafd3e9e1c9f","IPY_MODEL_3e41c1bb2fb14c3f8f05c2618fed5cc0"],"layout":"IPY_MODEL_0b74bcb88df645a38e25d399981bd29a"}},"e88c8956761649f187a75c5edd83b985":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f064c08fc4b4de4aa07da59675e5ffe","placeholder":"​","style":"IPY_MODEL_f80a273b06ce458f98acf54a45f40c61","value":""}},"8fbee034c0444fac9431fafd3e9e1c9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec43530f4ace455fb918c7408b0a2208","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17214080ff3d4a59aa4e356464f1da03","value":0}},"3e41c1bb2fb14c3f8f05c2618fed5cc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f123e022e70a4911a4add8cfc12b58b7","placeholder":"​","style":"IPY_MODEL_426fbe97634743d68c576ba2d2749d9a","value":" 0/0 [00:00&lt;?, ?it/s]"}},"0b74bcb88df645a38e25d399981bd29a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f064c08fc4b4de4aa07da59675e5ffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f80a273b06ce458f98acf54a45f40c61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec43530f4ace455fb918c7408b0a2208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"17214080ff3d4a59aa4e356464f1da03":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f123e022e70a4911a4add8cfc12b58b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"426fbe97634743d68c576ba2d2749d9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75422a4f82224abda2b3c32814e237fe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e8ad5dfd996435cb4e74f4166d7b7da","IPY_MODEL_01cc731572df482283aa16781683efc4","IPY_MODEL_0fa39091ad1548cb90766e51e642d213"],"layout":"IPY_MODEL_ed8947663dcd4fe99d7a24a2a3c90d34"}},"6e8ad5dfd996435cb4e74f4166d7b7da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d7e0bf6aa684b049ed342a9863141df","placeholder":"​","style":"IPY_MODEL_da837fff2d5d473fb125f64c16ee435d","value":"preprocessor_config.json: 100%"}},"01cc731572df482283aa16781683efc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9840b4382b194eedaa8c57a489e032dc","max":316,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab78d756318148eb9338827eec6e76d3","value":316}},"0fa39091ad1548cb90766e51e642d213":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7b7c6d29214395ae77ab64d0040857","placeholder":"​","style":"IPY_MODEL_a08e756ee0274299978de82eca8b243d","value":" 316/316 [00:00&lt;00:00, 27.3kB/s]"}},"ed8947663dcd4fe99d7a24a2a3c90d34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d7e0bf6aa684b049ed342a9863141df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da837fff2d5d473fb125f64c16ee435d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9840b4382b194eedaa8c57a489e032dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab78d756318148eb9338827eec6e76d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f7b7c6d29214395ae77ab64d0040857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a08e756ee0274299978de82eca8b243d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d5b2585c904f7184b9e0bc2f46f584":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fcb83e298075457299d9796301b9f9a5","IPY_MODEL_c2b8f1acdd6c4706870f42fd9dae10c0","IPY_MODEL_b354ac12d32946829df7b884c2270d0d"],"layout":"IPY_MODEL_fa0efc55e46042b7bb967ca9ee945e5b"}},"fcb83e298075457299d9796301b9f9a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_117d61ec7191465f9d2bd0297ac1f1da","placeholder":"​","style":"IPY_MODEL_4dabacd878554635bde1596976b9c9cf","value":"config.json: 100%"}},"c2b8f1acdd6c4706870f42fd9dae10c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bebf195820154060910463c44f474dda","max":4757,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4985ea126da541afb866606a602b9329","value":4757}},"b354ac12d32946829df7b884c2270d0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f1cddb411b74218982fd5a2f9673fc3","placeholder":"​","style":"IPY_MODEL_0797889aed30457abeb9c469d8ba3383","value":" 4.76k/4.76k [00:00&lt;00:00, 360kB/s]"}},"fa0efc55e46042b7bb967ca9ee945e5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"117d61ec7191465f9d2bd0297ac1f1da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dabacd878554635bde1596976b9c9cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bebf195820154060910463c44f474dda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4985ea126da541afb866606a602b9329":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f1cddb411b74218982fd5a2f9673fc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0797889aed30457abeb9c469d8ba3383":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91f4599328604b5a84ddb9ab389eed21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff34521141144787ac8303ce316a556b","IPY_MODEL_eeb7df54529346288674d8000c565e21","IPY_MODEL_a62ee844eeca4e29bfc2fc0797328b75"],"layout":"IPY_MODEL_e60767d5f56f4457be27ba461de9a1de"}},"ff34521141144787ac8303ce316a556b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f76e11e458df44dca43be52413c63a9b","placeholder":"​","style":"IPY_MODEL_341ca9bdf4de4d07b706de8efd5d30e1","value":"pytorch_model.bin: 100%"}},"eeb7df54529346288674d8000c565e21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dc91fd7606a43f8b175f450dd9ceb4e","max":1711974081,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7f1d508dd034f9389a31724aaf12451","value":1711974081}},"a62ee844eeca4e29bfc2fc0797328b75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f0087e6372f4a1f870918a3992f5c90","placeholder":"​","style":"IPY_MODEL_a169d4965efa4ab6821d0f48f1a9cf58","value":" 1.71G/1.71G [00:13&lt;00:00, 123MB/s]"}},"e60767d5f56f4457be27ba461de9a1de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f76e11e458df44dca43be52413c63a9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"341ca9bdf4de4d07b706de8efd5d30e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dc91fd7606a43f8b175f450dd9ceb4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7f1d508dd034f9389a31724aaf12451":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f0087e6372f4a1f870918a3992f5c90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a169d4965efa4ab6821d0f48f1a9cf58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f205967d036454bb63dbbb18dd8e960":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10342e9811184517b657edb62cb33842","IPY_MODEL_73776f61059f4849aab8fc66f4346aed","IPY_MODEL_aa49bcb8b514419da0d61b7ecf4fa4e2"],"layout":"IPY_MODEL_475362e440f14b96965ec7f2626c418d"}},"10342e9811184517b657edb62cb33842":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6553228d64934c608bebf48aa7269a2d","placeholder":"​","style":"IPY_MODEL_3477cae1fd7b4be4b1d46b7ec3141970","value":"Loading checkpoint shards: 100%"}},"73776f61059f4849aab8fc66f4346aed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8df155b3ae484d34b2c598a46191180e","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af709c4d21f941b28fe198bb5dd8942b","value":4}},"aa49bcb8b514419da0d61b7ecf4fa4e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e51ddd35121840b1bf8f83ca8990c732","placeholder":"​","style":"IPY_MODEL_4bc6ef91c16e4b44be1210a3f4b9cdb8","value":" 4/4 [01:10&lt;00:00, 14.57s/it]"}},"475362e440f14b96965ec7f2626c418d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6553228d64934c608bebf48aa7269a2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3477cae1fd7b4be4b1d46b7ec3141970":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8df155b3ae484d34b2c598a46191180e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af709c4d21f941b28fe198bb5dd8942b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e51ddd35121840b1bf8f83ca8990c732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bc6ef91c16e4b44be1210a3f4b9cdb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# SCENE ASSESSMENT WITH LLaVA\n","# Version: 1.0\n","# Git repo: https://github.com/perezjoan/SAGAI\n","# This script enables batch visual analysis of image datasets using LLaVA (Large\n","# Language and Vision Assistant) directly within Google Colab. It works in two\n","# parts: first, it clones the LLaVA source code from GitHub to set up the model\n","# architecture and logic; second, it downloads pretrained model weights\n","# (e.g. llava-v1.6-mistral-7b) from Hugging Face to actually run the model.\n","# Users can customize the prompt by defining a role, theory, task, and output\n","# format—allowing the model to follow a consistent and domain-specific evaluation\n","# protocol. The script then loops through all images in a Google Drive folder,\n","# applies the visual-language model to each image, and writes the structured\n","# responses into a unified text report. This makes it suitable for use cases\n","# such as walkability audits, architectural evaluations, and structured image\n","# interpretation workflows."],"metadata":{"id":"jJtOE2cL5jrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clone LLaVA and restart the session\n","%cd /content\n","!git clone https://github.com/haotian-liu/LLaVA.git\n","%cd LLaVA\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Qlmpeoc9kr-x","executionInfo":{"status":"ok","timestamp":1744036761056,"user_tz":-120,"elapsed":228742,"user":{"displayName":"joan perez","userId":"00601441442927158770"}},"outputId":"c1d9aead-1316-4dea-82b4-e6e9973ef4c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'LLaVA'...\n","remote: Enumerating objects: 2297, done.\u001b[K\n","remote: Total 2297 (delta 0), reused 0 (delta 0), pack-reused 2297 (from 1)\u001b[K\n","Receiving objects: 100% (2297/2297), 13.71 MiB | 11.91 MiB/s, done.\n","Resolving deltas: 100% (1404/1404), done.\n","/content/LLaVA\n","Obtaining file:///content/LLaVA\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch==2.1.2 (from llava==1.2.2.post1)\n","  Downloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting torchvision==0.16.2 (from llava==1.2.2.post1)\n","  Downloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Collecting transformers==4.37.2 (from llava==1.2.2.post1)\n","  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers==0.15.1 (from llava==1.2.2.post1)\n","  Downloading tokenizers-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting sentencepiece==0.1.99 (from llava==1.2.2.post1)\n","  Downloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Collecting shortuuid (from llava==1.2.2.post1)\n","  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n","Collecting accelerate==0.21.0 (from llava==1.2.2.post1)\n","  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (from llava==1.2.2.post1) (0.14.0)\n","Collecting bitsandbytes (from llava==1.2.2.post1)\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llava==1.2.2.post1) (2.11.1)\n","Collecting markdown2[all] (from llava==1.2.2.post1)\n","  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llava==1.2.2.post1) (2.0.2)\n","Collecting scikit-learn==1.2.2 (from llava==1.2.2.post1)\n","  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting gradio==4.16.0 (from llava==1.2.2.post1)\n","  Downloading gradio-4.16.0-py3-none-any.whl.metadata (15 kB)\n","Collecting gradio_client==0.8.1 (from llava==1.2.2.post1)\n","  Downloading gradio_client-0.8.1-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from llava==1.2.2.post1) (2.32.3)\n","Collecting httpx==0.24.0 (from llava==1.2.2.post1)\n","  Downloading httpx-0.24.0-py3-none-any.whl.metadata (8.1 kB)\n","Collecting uvicorn (from llava==1.2.2.post1)\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting fastapi (from llava==1.2.2.post1)\n","  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n","Collecting einops==0.6.1 (from llava==1.2.2.post1)\n","  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n","Collecting einops-exts==0.0.4 (from llava==1.2.2.post1)\n","  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n","Collecting timm==0.6.13 (from llava==1.2.2.post1)\n","  Downloading timm-0.6.13-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0->llava==1.2.2.post1) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0->llava==1.2.2.post1) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.21.0->llava==1.2.2.post1) (6.0.2)\n","Collecting aiofiles<24.0,>=22.0 (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (5.5.0)\n","Collecting ffmpy (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.30.1)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (6.5.2)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.1.6)\n","Collecting markupsafe~=2.0 (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.10.0)\n","Collecting numpy (from llava==1.2.2.post1)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.10.16)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.2.2)\n","Collecting pillow<11.0,>=8.0 (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n","Collecting pydub (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting python-multipart (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Collecting ruff>=0.1.7 (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n","Collecting semantic-version~=2.0 (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting tomlkit==0.12.0 (from gradio==4.16.0->llava==1.2.2.post1)\n","  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: typer<1.0,>=0.9 in /usr/local/lib/python3.11/dist-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.15.2)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.16.0->llava==1.2.2.post1) (4.13.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio_client==0.8.1->llava==1.2.2.post1) (2025.3.2)\n","Collecting websockets<12.0,>=10.0 (from gradio_client==0.8.1->llava==1.2.2.post1)\n","  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava==1.2.2.post1) (2025.1.31)\n","Collecting httpcore<0.18.0,>=0.15.0 (from httpx==0.24.0->llava==1.2.2.post1)\n","  Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava==1.2.2.post1) (3.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.24.0->llava==1.2.2.post1) (1.3.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (1.14.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (3.6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava==1.2.2.post1) (3.18.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava==1.2.2.post1) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2->llava==1.2.2.post1) (3.4.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.1.0 (from torch==2.1.2->llava==1.2.2.post1)\n","  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->llava==1.2.2.post1) (2024.11.6)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->llava==1.2.2.post1) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2->llava==1.2.2.post1) (4.67.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->llava==1.2.2.post1) (12.5.82)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llava==1.2.2.post1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llava==1.2.2.post1) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llava==1.2.2.post1) (0.4.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llava==1.2.2.post1) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llava==1.2.2.post1) (0.14.0)\n","Collecting starlette<0.47.0,>=0.40.0 (from fastapi->llava==1.2.2.post1)\n","  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: pygments>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from markdown2[all]->llava==1.2.2.post1) (2.18.0)\n","Collecting wavedrom (from markdown2[all]->llava==1.2.2.post1)\n","  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting latex2mathml (from markdown2[all]->llava==1.2.2.post1)\n","  Downloading latex2mathml-3.77.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->llava==1.2.2.post1) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->llava==1.2.2.post1) (2.3.0)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (4.23.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (1.33.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.2.2.post1) (4.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava==1.2.2.post1) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava==1.2.2.post1) (2025.2)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (13.9.4)\n","\u001b[33mWARNING: typer 0.15.2 does not provide the extra 'all'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2->llava==1.2.2.post1) (1.3.0)\n","Collecting svgwrite (from wavedrom->markdown2[all]->llava==1.2.2.post1)\n","  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from wavedrom->markdown2[all]->llava==1.2.2.post1) (1.17.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.24.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.1.2)\n","Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n","Downloading gradio-4.16.0-py3-none-any.whl (16.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gradio_client-0.8.1-py3-none-any.whl (305 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpx-0.24.0-py3-none-any.whl (75 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentencepiece-0.1.99-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-0.6.13-py3-none-any.whl (549 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.1.2-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.16.2-cp311-cp311-manylinux1_x86_64.whl (6.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n","Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n","Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n","Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llava, wavedrom\n","  Building editable for llava (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llava: filename=llava-1.2.2.post1-0.editable-py3-none-any.whl size=17891 sha256=03175cf39f820b2acf9d7751839b6ac75f58afd87d7078dbb17b027f0863257f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-07h83avg/wheels/55/a1/dd/fc37ed4f847b25c140c977a5e08209dfbe50be519f62fb82a9\n","  Building wheel for wavedrom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30084 sha256=26b3aa58adf89b0264161870341ca8aaad7637a7a8a2e8a4b16c1e81c759d86e\n","  Stored in directory: /root/.cache/pip/wheels/23/cf/3b/4dcf6b22fa41c5ece715fa5f4e05afd683e7b0ce0f2fcc7bb6\n","Successfully built llava wavedrom\n","Installing collected packages: sentencepiece, pydub, websockets, uvicorn, triton, tomlkit, svgwrite, shortuuid, semantic-version, ruff, python-multipart, pillow, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, markupsafe, markdown2, latex2mathml, ffmpy, einops, aiofiles, wavedrom, starlette, nvidia-cusolver-cu12, nvidia-cudnn-cu12, httpcore, einops-exts, torch, tokenizers, scikit-learn, httpx, fastapi, transformers, torchvision, gradio_client, bitsandbytes, accelerate, timm, gradio, llava\n","  Attempting uninstall: sentencepiece\n","    Found existing installation: sentencepiece 0.2.0\n","    Uninstalling sentencepiece-0.2.0:\n","      Successfully uninstalled sentencepiece-0.2.0\n","  Attempting uninstall: websockets\n","    Found existing installation: websockets 15.0.1\n","    Uninstalling websockets-15.0.1:\n","      Successfully uninstalled websockets-15.0.1\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: pillow\n","    Found existing installation: pillow 11.1.0\n","    Uninstalling pillow-11.1.0:\n","      Successfully uninstalled pillow-11.1.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: markupsafe\n","    Found existing installation: MarkupSafe 3.0.2\n","    Uninstalling MarkupSafe-3.0.2:\n","      Successfully uninstalled MarkupSafe-3.0.2\n","  Attempting uninstall: einops\n","    Found existing installation: einops 0.8.1\n","    Uninstalling einops-0.8.1:\n","      Successfully uninstalled einops-0.8.1\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: httpcore\n","    Found existing installation: httpcore 1.0.7\n","    Uninstalling httpcore-1.0.7:\n","      Successfully uninstalled httpcore-1.0.7\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.1\n","    Uninstalling tokenizers-0.21.1:\n","      Successfully uninstalled tokenizers-0.21.1\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: httpx\n","    Found existing installation: httpx 0.28.1\n","    Uninstalling httpx-0.28.1:\n","      Successfully uninstalled httpx-0.28.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.50.3\n","    Uninstalling transformers-4.50.3:\n","      Successfully uninstalled transformers-4.50.3\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 1.5.2\n","    Uninstalling accelerate-1.5.2:\n","      Successfully uninstalled accelerate-1.5.2\n","  Attempting uninstall: timm\n","    Found existing installation: timm 1.0.15\n","    Uninstalling timm-1.0.15:\n","      Successfully uninstalled timm-1.0.15\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n","google-genai 1.9.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.24.0 which is incompatible.\n","google-genai 1.9.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.2 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.21.0 aiofiles-23.2.1 bitsandbytes-0.45.5 einops-0.6.1 einops-exts-0.0.4 fastapi-0.115.12 ffmpy-0.5.0 gradio-4.16.0 gradio_client-0.8.1 httpcore-0.17.3 httpx-0.24.0 latex2mathml-3.77.0 llava-1.2.2.post1 markdown2-2.5.3 markupsafe-2.1.5 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 pillow-10.4.0 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 scikit-learn-1.2.2 semantic-version-2.10.0 sentencepiece-0.1.99 shortuuid-1.0.13 starlette-0.46.1 svgwrite-1.4.3 timm-0.6.13 tokenizers-0.15.1 tomlkit-0.12.0 torch-2.1.2 torchvision-0.16.2 transformers-4.37.2 triton-2.1.0 uvicorn-0.34.0 wavedrom-2.0.3.post3 websockets-11.0.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]},"id":"6320b9f3966e4d669687f0292e4bb8ec"}},"metadata":{}}]},{"cell_type":"code","source":["# ------------------------ USER PARAMETERS -------------------------------------\n","# Mount Google Drive and set user paths\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set your case study name and task number\n","case_study = \"nice\"  # e.g., \"vienna\", \"nice\", etc.\n","selected_task = \"T1\"   # Options: \"T1\" (CATEGORIZATION), \"T2\" (COUNTING), or \"T3\" (MEASURING)\n","display_images = False # Display images for visual verification (optional)\n","\n","# ------------------------ FOLDERS STRUCTURE -----------------------------------\n","\n","# Automatically build paths\n","root_path = f\"/content/drive/MyDrive/SAGAI\"\n","image_folder = os.path.join(root_path, f\"StreetViewBatchDownload_{case_study.capitalize()}\")\n","output_path = os.path.join(root_path, f\"Image_Analysis/Score_Analysis_LLaVA_{case_study.capitalize()}_{selected_task}.csv\")\n","# Ensure output directory exists\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"icL84sCxpEyF","executionInfo":{"status":"ok","timestamp":1744038216654,"user_tz":-120,"elapsed":945,"user":{"displayName":"joan perez","userId":"00601441442927158770"}},"outputId":"8ff88976-8b2e-4fe4-d498-ffde7d15a708"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["50636a4024c04d479878c6be351763b3","f2b7756953f54efe9a1bc5c42da276b9","8c3b8e9b60fe4462865dfeb5e14e9ec0","76b2f46ebe4645458985126e7e63dfed","853913df5c6447779d4aa34195d0583c","90795a55239c4b75ae1d2d21cb23bbe7","000280726eb04a7e832f1a28ba2b1221","d3ae89d705c4458384bb848176bc74c8","89ffdd727aec4c78a7bd8a65dbab539f","845488df38f3481dbcff8f3a6524997a","026f3fa09cd84c5ca73bcffa8c2f82ad","9d1043a152dc4070b78bc19077672367","b0b80960820d44d4a765f03c2c9cf9cf","4c4920ed0de74190a1ee5b4786988e59","02a7ec249f5f49579ff199b86e979a95","ae7147790ac8439983e422c722255f3f","05ded39522ad47b69e003c65bf17f8d9","a68efc5e55f74028b21bb0abb9ee3d6d","b48c82398f164c6a8d471ddb981a52fd","fdb5ff6923a64656b259a3428a8817c5","2affa17c43b14e38874c787817425273","41d357b5a6ff4c1e9e829c2f4bac2a4d","4ac9c1803eed444d93d13da60934484c","fdded508c8d14bedbcb04fdd7653e033","e78156ba1016460eaa212d38433a843e","31c12294cb4a4551afa10f724b4f78c9","bf4a6c798411424e9b9073dc562eef73","650f9eabf5d34fe8a69ca4b40bf76d28","cc19a044a9b44af194cd575281507951","1a272646edda438fae53504bb9de4c45","0597573c70284bb3b1da892aaea1d28d","341d8026b2024164a087277225055db6","df7a7dad83294f82b1a466d25e02fc36","cd6c656f09bf4b198c7dd9691472eff0","96a40eae71ca4d11a96ae096386c04c2","4819381fefe44c59bbbec85aeba16943","495de7a018cc42b1a03db0fa47c3b9ae","64fce670b1c64cca9375dfb8f1cc9423","839a3f251cc848a99d7b631fe52566a2","a819db7b0fbe4cf582bcfafaf2c3fcc6","099d29749fb24c54b1e412ca5b3f55da","300c7f8c64ad49b0847e5b287de26d90","b9422d76bb4e460f8b6e70868fcc79ed","407e21c2647e49ee94fd1624ef01890e","c3cfa3c5801648bb965c720e66ee0966","53a5334eb76f448c8571eb69c157e91c","d317b38fb99844a9b386cd515aab93ac","6885e431881048ef856121eb18002d84","1b6b054b23a84abea8e2e1ec4ac3c486","492a8cc500ec489bb0766478fbd91a7d","faf8de4f36df46288d532d6cfbf6aaa0","06ac742458a84938adac0c5aee3eadb2","2884e5a46549494ab747c327590f2fdd","d2e9e481b2d44ccc93122e7e591d4781","c8ba7ed43ebf474a96a39f82cdbb69fd","147eaf968dc44ccdb532360a533b57fe","cf3c918db85a4608a12fc562082bb39d","86af250fcd5b4b9293893423525a5b7b","d896d61704e844f4a0a88817395a4648","f7012699ae4a44ce8d447568c165409f","20157737fe8e4b7a8ca0068a1313b7be","f837ac64886f4bc6a270207fe3953d92","d228849e97824b8d93f686b1d787b990","4df3e5a94b104831aa2f5300789a899c","2ddf75f990e94d3d9fc2d8cae5281ee4","ae946acd5c7a482eb1ca0b586e7675f9","fd584dd7447343b985823295b8addd57","ab2e6463698040d5b76cfc46ca2ed25e","d6a52ecc482a49fba8f195b78fbd6b8a","899733af9d4b462fbee0b403a1e8285a","f14f76aa115f4ba9a9ae3634d117021a","f5d597e162ae4981ac4031ab13c3f350","4484ee3dfe4d46e1a0026cc7c5b9e621","5b682a8863c14943a60cf4b7faf71c66","8fad3b2086184033ae5be47b3b46284d","1452c85569b24d55baf3e8c3077dc036","2b7adb1a804744a991abbdb3a9a07444","38c186c6b7494f539de491ba67f8b412","f3b1a7cf65e84186bcd4dbb26cb2a939","83d99181700249688187494a00d705c0","dab80ba46229491baa86c41fe6a3cf2f","075f8d1447bb463193456791f7717489","105b970eceb745f6a6b3158d6f9f0700","1284f2b225a04535ba4b930ccfe65108","90476f3a082c41c581681b81bcbad962","7a6d74cdc900401fa0c30a2f558e06a3","4fc74588742e406b946a9251f87a2154","2cc9b798d1bd4fc2bc1b49c22d1105dd","2ca5cb317f7f42578dc09ba66f7003bc","3126b38360a340d4affcbf46119a4190","7e1d709f38e5417d9c8e35019506011a","8fbff9a004b040de9231229c2b93294a","51e669e68eaf4d30899bb7b75dcc4369","e753e715a8cf46629a7a6f187396a256","c33b337c48654d1d828b3fddf6d76893","7ee1d65a2990465fbcf2bd6442f67389","4c2026a284134ca7b27ffc1403d9c9a8","98775e8301d74ee7b5020121e0fa45a4","4e3d924660e544d5b70c34321d3c57b0","c07e9e75c215489baf5472807776b4ff","f51acd090c32452ab27954101099a8ff","0898e438e5454f58ad2e9238ee48a5cc","f6d11992b2594987b6336ec4470fe8ba","28c55f0686e04a38959d1993761eb07e","3c66c7abc3914f21b19e000fe7bf2a25","5a4f8fc215bf4294bc4dbc71413272bf","05338600ca854e1290639be39f35796c","a32ba3cb5b7c452fabcd311c92bc1db9","0a5538c5e68a4cedaed4cb8a526a64d3","034fc232290e4ee6b1492e47e1f76680","acb9d669653749869038f3ed25fbcc22","a70c84be45e74dc282e45d91bb97ea6a","5b8d20cb53f54e978fafeed5431732e9","4aee1faa206d46239c9e528eba4e5073","de0c778950944eebbc3bba1515a71fca","54337a2b77534223b0497ffc2b83aab9","a5af955d02bf43a4b10a800e22fac2b4","2adec48a433a4d7bac0f949cf4cf5462","bd702378f8254418a8fe1c6a1722e8f7","1b4eae7b55b34979aeac6914ca05d42e","b1602c7388b841e39e7c4a11a7b27a77","d6cd9babfe864958ab4c489d70054f15","42494d4b673f447f9d735c35cc3c799f","20e1bdeff8ce4337abdde2d5eb388fe0","ab8928d5727c4afbbbcad5e2410cc670","f12004307d394ca68aacd0f5848a38c6","6d4864d0096a4b86ab8a623b22912ec7","311196ea168043bb9dbe0963f6803403","e321851490d24e7896fbde4ebb5e9b08","1c5914c36dc3402abdd5fcb9a4e8014c","0935f2f55fa44bb4915990b16f29e7c4","7262a607c3ef4b96b934b3ddd2c243c7","2192e15597874e34ace4b95403d54989","592c40ccfd52456da025f6bf027c39f4","99380a4b2f924dd78458933194e43d9b","84085e22df164fb88be71c23abb6c397","cba1f91d78c34dc0b9409d4a32c5e1f1","30b5f40b8bc9498690004a74497cd588","dd1bc4fdbfac4f03a9e8a2488c713b5e","a12bd35fdd3f4bb4ba54e1bfa168237f","72fdfd6f1ed14c809f9bb70a26c1608e","754625e390d449a6bae4c691332b5dbc","1f0cdc0366b1493aa175f3cbbb198b54","264825367b4b4a3b96723d5016d78d25","aecd05b91061468f961a776d58a045ef","a956c28fbd3e4c08b1c0a5954167a8cf","2991fc08b535483f8c02f2afde23e1dc","6b44e9e7645546508af4eedbc4c2e50a","17310ac1a63643099b9a56e9839be005","52e179ef704846e68423ed63c57c16ba","009a9b2ce341411bbbb488b22e38b13a","50f185dd759549358abe7365e89d19e2","b40eb985841a4276a61b933f3d51805a","3069e22cfc944262b408d1a0eaafeff2","0e523ad4d9a8498eac1184c75c600d23","b5f30b7158214b8e8329c4d032f2617f","f62b4f4c103240aaa9024e495d75bd3e","3fb3b925fa74450d919db88a6696b6ef","396a2368fdcc4fb9aa76564b752927fa","adcc1d52a0294116aefb724daba5edac","8ed37aae288145149123cd3821999791","9257d191dc84410cb8435d8f716e6d88","ce747633e4064f0c87f083d59f1214e0","8e9c200c8a0a44dcbb5b1d28cf47c4dc","794a1a5cd86245bcb8837264de5b14b8","0b6c1da480b84f76bc17275379ec174a","f0f0ad41d96d482995a1d9d99ba2a272","c6bbb3b27dde4970bf3f4728a339512d","ed6b9ef4f20d4f23a1775bb0a984b7cc","8efc5cdc20784ac5b8693338a30978c8","da9c0897fecd415c8c52ab07bfcc26b8","cf10b96dec594a60989067ad3ad20242","c976f7b557884d15afd4b8ed1269fbd8","11e6603b0dde4e9b8e2c50fd26d3b9f3","8e7217f9ca55461aa48de4342a2f6d90","6218a4e0aa854d8db2830db48827cca8","28b055d0a0f545f2b83ec412ef747da2","e88c8956761649f187a75c5edd83b985","8fbee034c0444fac9431fafd3e9e1c9f","3e41c1bb2fb14c3f8f05c2618fed5cc0","0b74bcb88df645a38e25d399981bd29a","7f064c08fc4b4de4aa07da59675e5ffe","f80a273b06ce458f98acf54a45f40c61","ec43530f4ace455fb918c7408b0a2208","17214080ff3d4a59aa4e356464f1da03","f123e022e70a4911a4add8cfc12b58b7","426fbe97634743d68c576ba2d2749d9a","75422a4f82224abda2b3c32814e237fe","6e8ad5dfd996435cb4e74f4166d7b7da","01cc731572df482283aa16781683efc4","0fa39091ad1548cb90766e51e642d213","ed8947663dcd4fe99d7a24a2a3c90d34","5d7e0bf6aa684b049ed342a9863141df","da837fff2d5d473fb125f64c16ee435d","9840b4382b194eedaa8c57a489e032dc","ab78d756318148eb9338827eec6e76d3","5f7b7c6d29214395ae77ab64d0040857","a08e756ee0274299978de82eca8b243d","17d5b2585c904f7184b9e0bc2f46f584","fcb83e298075457299d9796301b9f9a5","c2b8f1acdd6c4706870f42fd9dae10c0","b354ac12d32946829df7b884c2270d0d","fa0efc55e46042b7bb967ca9ee945e5b","117d61ec7191465f9d2bd0297ac1f1da","4dabacd878554635bde1596976b9c9cf","bebf195820154060910463c44f474dda","4985ea126da541afb866606a602b9329","9f1cddb411b74218982fd5a2f9673fc3","0797889aed30457abeb9c469d8ba3383","91f4599328604b5a84ddb9ab389eed21","ff34521141144787ac8303ce316a556b","eeb7df54529346288674d8000c565e21","a62ee844eeca4e29bfc2fc0797328b75","e60767d5f56f4457be27ba461de9a1de","f76e11e458df44dca43be52413c63a9b","341ca9bdf4de4d07b706de8efd5d30e1","2dc91fd7606a43f8b175f450dd9ceb4e","f7f1d508dd034f9389a31724aaf12451","9f0087e6372f4a1f870918a3992f5c90","a169d4965efa4ab6821d0f48f1a9cf58","0f205967d036454bb63dbbb18dd8e960","10342e9811184517b657edb62cb33842","73776f61059f4849aab8fc66f4346aed","aa49bcb8b514419da0d61b7ecf4fa4e2","475362e440f14b96965ec7f2626c418d","6553228d64934c608bebf48aa7269a2d","3477cae1fd7b4be4b1d46b7ec3141970","8df155b3ae484d34b2c598a46191180e","af709c4d21f941b28fe198bb5dd8942b","e51ddd35121840b1bf8f83ca8990c732","4bc6ef91c16e4b44be1210a3f4b9cdb8"]},"id":"ZCY3GXB3fLwr","executionInfo":{"status":"ok","timestamp":1744037196258,"user_tz":-120,"elapsed":326448,"user":{"displayName":"joan perez","userId":"00601441442927158770"}},"outputId":"1a30cf22-7180-4a4f-813f-31b50f9ab374"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50636a4024c04d479878c6be351763b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1043a152dc4070b78bc19077672367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac9c1803eed444d93d13da60934484c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd6c656f09bf4b198c7dd9691472eff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3cfa3c5801648bb965c720e66ee0966"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147eaf968dc44ccdb532360a533b57fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd584dd7447343b985823295b8addd57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/262M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c186c6b7494f539de491ba67f8b412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca5cb317f7f42578dc09ba66f7003bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c07e9e75c215489baf5472807776b4ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb9d669653749869038f3ed25fbcc22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6cd9babfe864958ab4c489d70054f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2192e15597874e34ace4b95403d54989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264825367b4b4a3b96723d5016d78d25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["trainer_state.json:   0%|          | 0.00/719k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e523ad4d9a8498eac1184c75c600d23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["training_args.bin:   0%|          | 0.00/7.04k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6c1da480b84f76bc17275379ec174a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b055d0a0f545f2b83ec412ef747da2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using a model of type llava_mistral to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75422a4f82224abda2b3c32814e237fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d5b2585c904f7184b9e0bc2f46f584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f4599328604b5a84ddb9ab389eed21"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f205967d036454bb63dbbb18dd8e960"}},"metadata":{}}],"source":["# -------------- LOAD THE MODEL  ----------------------\n","# Download LLaVA Model and weights from Hugging Face\n","from huggingface_hub import snapshot_download\n","snapshot_download(repo_id=\"liuhaotian/llava-v1.6-mistral-7b\", local_dir=\"/content/lllava-v1.6-mistral-7b\")\n","model_path = \"/content/lllava-v1.6-mistral-7b\"\n","from transformers import AutoTokenizer, BitsAndBytesConfig\n","from llava.utils import disable_torch_init\n","from llava.model import LlavaLlamaForCausalLM\n","import torch\n","\n","# Quantization config (4-bit)\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type='nf4'\n",")\n","\n","# Load model and tokenizer from local folder\n","model = LlavaLlamaForCausalLM.from_pretrained(\n","    model_path,\n","    device_map=\"auto\",\n","    low_cpu_mem_usage=True,\n","    quantization_config=quant_config\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n","\n","# Load and prepare the vision tower (for image encoding)\n","vision_tower = model.get_vision_tower()\n","if not vision_tower.is_loaded:\n","    vision_tower.load_model()\n","vision_tower.to(device=\"cuda\")\n","image_processor = vision_tower.image_processor"]},{"cell_type":"code","source":["# -------------------------- IMAGE ANALYSIS FUNCTION --------------------------\n","# This function sends an image + prompt to the LLaVA model and returns the response.\n","# It uses LLaVA's internal conversation templates and image preprocessing tools.\n","\n","from IPython.display import display\n","from llava.conversation import conv_templates, SeparatorStyle\n","from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n","from llava.utils import disable_torch_init\n","from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria\n","import os\n","from PIL import Image\n","\n","def caption_image(image_file, prompt):\n","    if image_file.startswith('http'): # Load image from file path or URL\n","        response = requests.get(image_file)\n","        image = Image.open(BytesIO(response.content)).convert('RGB')\n","    else:\n","        image = Image.open(image_file).convert('RGB')\n","    # Disable default torch weight initialization for reproducibility\n","    disable_torch_init()\n","    # Select LLaVA's conversation format\n","    conv_mode = \"llava_v0\"\n","    conv = conv_templates[conv_mode].copy()\n","    roles = conv.roles\n","\n","    # Preprocess image and move to GPU\n","    image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'].half().cuda()\n","\n","    # Build prompt with special tokens and user input\n","    inp = f\"{roles[0]}: {prompt}\"\n","    inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n","\n","    # Fill conversation structure\n","    conv.append_message(conv.roles[0], inp)\n","    conv.append_message(conv.roles[1], None)\n","\n","    # Tokenize full prompt (with image tokens)\n","    raw_prompt = conv.get_prompt()\n","    input_ids = tokenizer_image_token(\n","        raw_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n","    ).unsqueeze(0).cuda()\n","\n","    # Set stopping condition using template-defined separator\n","    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n","    stopping_criteria = KeywordsStoppingCriteria([stop_str], tokenizer, input_ids)\n","\n","    # Run model inference with mild randomness for better variety\n","    with torch.inference_mode():\n","        output_ids = model.generate(\n","            input_ids,\n","            images=image_tensor,\n","            do_sample=True,               # Enable sampling (adds variation)\n","            temperature=0.3,              # Low temperature for controlled creativity\n","            top_p=0.9,                    # Nucleus sampling within top 90% probable tokens\n","            max_new_tokens=10,           # Expecting short numeric response\n","            stopping_criteria=[stopping_criteria],\n","            pad_token_id=tokenizer.eos_token_id,\n","            eos_token_id=tokenizer.eos_token_id\n","        )\n","\n","    # Decode and clean output\n","    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n","    response = output_text.split(roles[1] + \":\")[-1].strip()\n","\n","    return response if response else \"[No output generated]\""],"metadata":{"id":"VFA8YWNejErR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TASK 1: CATEGORIZATION\n","\n","# ------------------------ TASK PROMPT SELECTION ------------------------\n","# TASK 1: CATEGORIZATION\n","if selected_task == \"T1\":\n","    role_description = (\n","        \"You are an AI trained to visually analyze street-level images. \"\n","        \"Your task is to determine whether the environment shown in the image is urban or rural.\"\n","    )\n","    theory_model = (\n","        \"Classification Guide:\\n\"\n","        \"- 0: Rural area — sparse built environment, natural surroundings, few or no buildings.\\n\"\n","        \"- 1: Urban area — dense built environment, visible infrastructure, buildings.\"\n","    )\n","    task = (\n","        \"Carefully observe the image and determine whether it depicts a rural or urban environment.\\n\"\n","        \"Use the classification guide above to assign a score.\\n\"\n","        \"Return only the classification (0 or 1). Do not explain your answer or add extra text.\"\n","    )\n","    response_format = \"Answer format: 0 or 1\"\n","# TASK 2: COUNTING\n","elif selected_task == \"T2\":\n","    role_description = (\n","        \"You are an AI trained to visually analyze street-level images. \"\n","        \"Your job is to detect the presence of commercial storefronts, such as shops, restaurants, or businesses.\"\n","    )\n","    theory_model = (\n","        \"Scoring Guide:\\n\"\n","        \"- 0: No visible shops or commercial storefronts.\\n\"\n","        \"- 1: One visible shop or storefront.\\n\"\n","        \"- 2: More than one shop or storefront is visible.\"\n","    )\n","    task = (\n","        \"Look at the image carefully and apply the scoring guide above.\\n\"\n","        \"Return only the score (0, 1, or 2) based on how many shops are visible.\\n\"\n","        \"Do not explain your answer or add text. Only output the number.\"\n","    )\n","    response_format = \"Answer format: 0, 1, or 2\"\n","# TASK 3: MEASURING\n","elif selected_task == \"T3\":\n","    role_description = (\n","        \"You are an AI trained to visually analyze street-level images. \"\n","        \"Your task is to estimate the visible width of a sidewalk.\"\n","    )\n","    theory_model = (\n","        \"Scoring Guide:\\n\"\n","        \"- 0: No visible sidewalk or the sidewalk is not clearly identifiable.\\n\"\n","        \"- Otherwise: Return the estimated width of the sidewalk in meters, rounded to the nearest 0.5 (e.g., 1.0, 1.5, 2.0, 2.5, 3.0).\"\n","    )\n","    task = (\n","        \"Look at the image carefully. If a sidewalk is visible, estimate its width in meters.\\n\"\n","        \"If no sidewalk is visible or it's unclear, return 0.\\n\"\n","        \"Do not explain your answer or add any text. Only output a single number.\"\n","    )\n","    response_format = \"Answer format: 0 or a number (e.g., 1.0, 1.5, 2.0, 2.5, 3.0)\"\n","\n","else:\n","    raise ValueError(\"Invalid selected_task. Choose from: 'T1', 'T2', or 'T3'.\")\n","\n","# Combine into full prompt\n","prompt = f\"\"\"\n","{role_description}\n","{theory_model}\n","{task}\n","{response_format}\n","\"\"\""],"metadata":{"id":"l76cwGdtfPmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import time\n","import pandas as pd\n","\n","# Start timer\n","start_time = time.time()\n","\n","# ------------------ Load already processed image names ------------------\n","already_processed = set()\n","\n","# If file exists, load existing rows\n","if os.path.exists(output_path) and os.stat(output_path).st_size > 0:\n","    df_existing = pd.read_csv(output_path)\n","    already_processed = set(df_existing['image_name'].tolist())\n","    print(f\"🔁 Resuming from previous run. Already processed: {len(already_processed)} images.\")\n","else:\n","    already_processed = set()\n","    print(\"🆕 Starting fresh. No existing results or empty file.\")\n","\n","# ------------------ Open CSV for appending ------------------\n","with open(output_path, 'a', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","\n","    # If file was just created, write header\n","    if os.stat(output_path).st_size == 0:\n","        writer.writerow([\"image_name\", \"score\"])  # header\n","\n","    for fname in sorted(os.listdir(image_folder)):\n","        if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n","            continue\n","\n","        if fname in already_processed:\n","            continue  # Skip already processed\n","\n","        image_path = os.path.join(image_folder, fname)\n","\n","        if \"_NA\" in fname:\n","            print(f\"⚠️ Skipping analysis for missing imagery: {fname}\")\n","            writer.writerow([fname, \"NA\"])\n","            continue\n","\n","        print(f\"\\n🔍 Processing: {fname}\")\n","        try:\n","            result = caption_image(image_path, prompt)\n","            print(f\"📝 Score: {result}\\n\")\n","\n","            # Conditionally display image\n","            if display_images:\n","                image = Image.open(image_path)\n","                display(image)\n","\n","            writer.writerow([fname, result.strip()])\n","\n","        except Exception as e:\n","            print(f\"❌ Error processing {fname}: {e}\")\n","            writer.writerow([fname, f\"ERROR: {e}\"])\n","\n","# ------------------ Summary ------------------\n","elapsed_time = time.time() - start_time\n","print(\"\\n✅ Scoring completed.\")\n","print(f\"📁 Scores saved in: {output_path}\")\n","print(f\"⏱️ Total runtime: {elapsed_time:.2f} seconds\")"],"metadata":{"id":"-3fMBya6vaPN","executionInfo":{"status":"ok","timestamp":1744038962636,"user_tz":-120,"elapsed":738757,"user":{"displayName":"joan perez","userId":"00601441442927158770"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae838418-460a-4f72-c2da-362dff480a74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["🆕 Starting fresh. No existing results or empty file.\n","\n","🔍 Processing: point_100_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_100_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_100_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_100_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_101_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_101_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_101_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_101_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_102_0.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_102_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_102_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_102_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_103_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_103_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_103_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_103_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_104_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_104_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_104_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_104_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_105_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_105_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_105_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_105_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_106_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_106_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_106_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_106_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_107_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_107_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_107_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_107_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_108_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_108_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_108_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_108_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_109_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_109_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_109_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_109_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_10_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_10_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_10_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_10_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_110_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_110_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_110_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_110_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_111_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_111_180.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_111_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_111_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_112_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_112_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_112_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_112_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_113_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_113_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_113_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_113_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_114_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_114_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_114_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_114_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_115_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_115_180.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_115_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_115_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_116_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_116_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_116_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_116_90.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_117_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_117_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_117_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_117_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_118_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_118_180.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_118_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_118_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_119_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_119_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_119_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_119_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_11_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_11_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_11_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_11_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_120_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_120_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_120_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_120_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_121_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_121_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_121_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_121_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_122_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_122_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_122_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_122_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_123_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_123_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_123_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_123_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_124_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_124_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_124_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_124_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_125_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_125_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_125_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_125_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_126_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_126_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_126_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_126_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_127_0.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_127_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_127_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_127_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_128_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_128_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_128_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_128_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_129_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_129_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_129_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_129_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_12_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_12_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_12_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_12_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_130_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_130_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_130_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_130_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_131_0.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_131_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_131_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_131_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_132_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_132_180.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_132_270.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_132_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_133_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_133_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_133_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_133_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_134_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_134_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_134_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_134_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_135_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_135_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_135_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_135_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_136_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_136_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_136_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_136_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_137_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_137_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_137_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_137_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_138_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_138_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_138_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_138_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_139_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_139_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_139_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_139_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_13_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_13_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_13_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_13_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_140_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_140_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_140_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_140_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_141_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_141_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_141_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_141_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_142_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_142_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_142_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_142_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_143_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_143_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_143_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_143_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_144_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_144_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_144_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_144_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_145_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_145_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_145_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_145_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_146_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_146_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_146_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_146_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_147_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_147_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_147_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_147_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_148_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_148_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_148_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_148_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_149_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_149_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_149_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_149_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_14_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_14_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_14_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_14_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_150_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_150_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_150_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_150_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_151_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_151_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_151_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_151_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_152_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_152_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_152_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_152_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_153_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_153_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_153_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_153_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_154_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_154_180.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_154_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_154_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_155_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_155_180.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_155_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_155_90.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_156_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_156_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_156_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_156_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_157_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_157_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_157_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_157_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_158_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_158_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_158_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_158_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_159_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_159_180.jpg\n","📝 Score: 1\n","\n","\n","🔍 Processing: point_159_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_159_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_15_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_15_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_15_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_15_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_160_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_160_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_160_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_160_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_161_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_161_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_161_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_161_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_162_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_162_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_162_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_162_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_163_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_163_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_163_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_163_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_164_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_164_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_164_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_164_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_165_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_165_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_165_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_165_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_16_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_16_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_16_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_16_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_17_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_17_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_17_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_17_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_18_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_18_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_18_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_18_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_19_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_19_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_19_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_19_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_1_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_1_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_1_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_1_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_20_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_20_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_20_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_20_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_21_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_21_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_21_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_21_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_22_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_22_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_22_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_22_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_23_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_23_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_23_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_23_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_24_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_24_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_24_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_24_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_25_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_25_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_25_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_25_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_26_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_26_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_26_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_26_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_27_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_27_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_27_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_27_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_28_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_28_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_28_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_28_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_29_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_29_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_29_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_29_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_2_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_2_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_2_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_2_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_30_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_30_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_30_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_30_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_31_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_31_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_31_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_31_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_32_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_32_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_32_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_32_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_33_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_33_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_33_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_33_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_34_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_34_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_34_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_34_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_35_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_35_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_35_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_35_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_36_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_36_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_36_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_36_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_37_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_37_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_37_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_37_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_38_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_38_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_38_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_38_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_39_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_39_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_39_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_39_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_3_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_3_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_3_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_3_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_40_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_40_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_40_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_40_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_41_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_41_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_41_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_41_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_42_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_42_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_42_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_42_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_43_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_43_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_43_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_43_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_44_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_44_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_44_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_44_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_45_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_45_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_45_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_45_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_46_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_46_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_46_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_46_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_47_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_47_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_47_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_47_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_48_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_48_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_48_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_48_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_49_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_49_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_49_270.jpg\n","📝 Score: 1\n","\n","\n","🔍 Processing: point_49_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_4_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_4_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_4_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_4_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_50_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_50_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_50_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_50_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_51_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_51_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_51_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_51_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_52_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_52_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_52_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_52_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_53_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_53_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_53_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_53_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_54_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_54_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_54_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_54_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_55_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_55_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_55_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_55_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_56_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_56_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_56_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_56_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_57_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_57_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_57_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_57_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_58_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_58_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_58_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_58_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_59_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_59_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_59_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_59_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_5_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_5_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_5_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_5_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_60_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_60_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_60_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_60_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_61_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_61_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_61_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_61_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_62_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_62_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_62_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_62_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_63_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_63_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_63_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_63_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_64_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_64_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_64_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_64_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_65_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_65_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_65_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_65_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_66_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_66_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_66_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_66_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_67_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_67_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_67_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_67_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_68_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_68_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_68_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_68_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_69_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_69_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_69_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_69_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_6_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_6_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_6_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_6_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_70_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_70_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_70_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_70_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_71_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_71_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_71_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_71_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_72_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_72_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_72_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_72_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_73_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_73_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_73_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_73_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_74_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_74_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_74_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_74_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_75_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_75_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_75_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_75_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_76_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_76_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_76_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_76_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_77_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_77_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_77_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_77_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_78_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_78_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_78_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_78_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_79_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_79_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_79_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_79_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_7_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_7_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_7_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_7_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_80_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_80_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_80_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_80_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_81_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_81_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_81_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_81_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_82_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_82_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_82_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_82_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_83_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_83_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_83_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_83_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_84_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_84_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_84_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_84_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_85_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_85_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_85_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_85_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_86_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_86_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_86_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_86_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_87_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_87_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_87_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_87_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_88_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_88_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_88_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_88_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_89_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_89_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_89_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_89_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_8_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_8_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_8_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_8_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_90_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_90_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_90_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_90_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_91_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_91_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_91_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_91_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_92_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_92_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_92_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_92_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_93_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_93_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_93_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_93_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_94_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_94_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_94_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_94_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_95_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_95_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_95_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_95_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_96_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_96_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_96_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_96_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_97_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_97_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_97_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_97_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_98_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_98_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_98_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_98_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_99_0.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_99_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_99_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_99_90.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_9_0.jpg\n","📝 Score: 2\n","\n","\n","🔍 Processing: point_9_180.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_9_270.jpg\n","📝 Score: 0\n","\n","\n","🔍 Processing: point_9_90.jpg\n","📝 Score: 2\n","\n","\n","✅ Scoring completed.\n","📁 Scores saved in: /content/drive/MyDrive/SAGAI/Image_Analysis/Score_Analysis_LLaVA_Vienna2_T2.csv\n","⏱️ Total runtime: 738.78 seconds\n"]}]}]}